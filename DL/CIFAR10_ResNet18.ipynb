{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRADHH7dsDAa9FMrW9Gh7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavt1605/Projects/blob/main/DL/CIFAR10_ResNet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_iXKtXgGdIcR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "print(\"Mean: \", mean)\n",
        "print(\"Standard Deviation: \", std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csrZFrK8fjZ4",
        "outputId": "fea2ca24-05c1-4deb-df3b-ca5b5e46ea57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:  (0.4914, 0.4822, 0.4465)\n",
            "Standard Deviation:  (0.2023, 0.1994, 0.201)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding= 4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.2, 0.2 ,0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform= transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ],
      "metadata": {
        "id": "kArW-y7oeUQn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_train= CIFAR10(root=\"data\", train=True, download=True, transform= train_transform)\n",
        "base_val= CIFAR10(root=\"data\", train=True, download=True, transform= test_transform)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "train_set, _ = torch.utils.data.random_split(base_train, [45000, 5000])\n",
        "_, val_set = torch.utils.data.random_split(base_val, [45000, 5000])\n",
        "\n",
        "test_set = CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=64, shuffle = False)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle = False)\n",
        "\n",
        "print(len(train_set), len(test_set), len(val_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCCeKQPof2AR",
        "outputId": "8741bca0-7c58-487c-d677-c2f3941eb184"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45000 10000 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(weights=None)\n",
        "\n",
        "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model.maxpool = nn.Identity()\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)"
      ],
      "metadata": {
        "id": "dk_b_XV5gD2y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor= 0.5 ,patience= 3)\n"
      ],
      "metadata": {
        "id": "ePbVt0gahwPl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvbLbmZ0ibbC",
        "outputId": "d5706f2b-f202-4fc1-8f8c-a9ba750eeae8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): Identity()\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_function, optimizer):\n",
        "  model.train()\n",
        "  total_loss=0\n",
        "\n",
        "  for batch, (image, label) in enumerate(dataloader):\n",
        "    image= image.to(device)\n",
        "    label= label.to(device)\n",
        "\n",
        "    prediction= model(image)\n",
        "    loss= loss_function(prediction, label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss+= loss\n",
        "\n",
        "  avg_loss = total_loss/len(dataloader)\n",
        "  print(f\"Training average loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "JetjvKE9ij35"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader, model, loss_function):\n",
        "  model.eval()\n",
        "  total_loss= 0\n",
        "  correct= 0\n",
        "  total= 0\n",
        "  with torch.no_grad():\n",
        "    for image, label in dataloader:\n",
        "      image= image.to(device)\n",
        "      label= label.to(device)\n",
        "\n",
        "      pred = model(image)\n",
        "      loss= loss_function(pred, label)\n",
        "      total_loss+= loss\n",
        "\n",
        "      predicted_classes = pred.argmax(dim=1)\n",
        "      correct+= (predicted_classes == label).sum().item()\n",
        "      total+= label.size(0)\n",
        "\n",
        "  avg_loss= total_loss / len(dataloader)\n",
        "  accuracy= correct/total * 100\n",
        "  print(f\"Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "  return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "vbOIp7-ykR8-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_function):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in dataloader:\n",
        "            image= image.to(device)\n",
        "            label= label.to(device)\n",
        "\n",
        "            preds = model(image)\n",
        "            loss = loss_function(preds, label)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_classes = preds.argmax(dim=1)\n",
        "            correct += (predicted_classes == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "8DBJQUmIks78"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    train(train_loader, model, loss_function, optimizer)\n",
        "    val_accuracy = validate(val_loader, model, loss_function)\n",
        "    scheduler.step(val_accuracy) #No arguments with StepLR, ReduceLROnPlateau takes arguments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j-16tkpkbWE",
        "outputId": "0cf2ae1e-aec8-4e01-8eeb-a00479df10ea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n",
            "Training average loss: 1.4534\n",
            "Validation Loss: 1.1027, Validation Accuracy: 60.48%\n",
            "\n",
            "Epoch 2/30\n",
            "Training average loss: 1.0244\n",
            "Validation Loss: 0.8965, Validation Accuracy: 68.62%\n",
            "\n",
            "Epoch 3/30\n",
            "Training average loss: 0.8252\n",
            "Validation Loss: 0.7367, Validation Accuracy: 74.48%\n",
            "\n",
            "Epoch 4/30\n",
            "Training average loss: 0.7084\n",
            "Validation Loss: 0.5944, Validation Accuracy: 79.36%\n",
            "\n",
            "Epoch 5/30\n",
            "Training average loss: 0.6260\n",
            "Validation Loss: 0.5497, Validation Accuracy: 81.64%\n",
            "\n",
            "Epoch 6/30\n",
            "Training average loss: 0.5614\n",
            "Validation Loss: 0.4664, Validation Accuracy: 84.14%\n",
            "\n",
            "Epoch 7/30\n",
            "Training average loss: 0.5105\n",
            "Validation Loss: 0.4360, Validation Accuracy: 85.08%\n",
            "\n",
            "Epoch 8/30\n",
            "Training average loss: 0.4714\n",
            "Validation Loss: 0.3575, Validation Accuracy: 88.18%\n",
            "\n",
            "Epoch 9/30\n",
            "Training average loss: 0.4415\n",
            "Validation Loss: 0.3317, Validation Accuracy: 88.40%\n",
            "\n",
            "Epoch 10/30\n",
            "Training average loss: 0.4137\n",
            "Validation Loss: 0.3143, Validation Accuracy: 89.62%\n",
            "\n",
            "Epoch 11/30\n",
            "Training average loss: 0.3809\n",
            "Validation Loss: 0.2853, Validation Accuracy: 90.40%\n",
            "\n",
            "Epoch 12/30\n",
            "Training average loss: 0.3584\n",
            "Validation Loss: 0.2456, Validation Accuracy: 91.78%\n",
            "\n",
            "Epoch 13/30\n",
            "Training average loss: 0.3349\n",
            "Validation Loss: 0.2529, Validation Accuracy: 92.00%\n",
            "\n",
            "Epoch 14/30\n",
            "Training average loss: 0.3142\n",
            "Validation Loss: 0.2232, Validation Accuracy: 92.70%\n",
            "\n",
            "Epoch 15/30\n",
            "Training average loss: 0.2982\n",
            "Validation Loss: 0.2382, Validation Accuracy: 92.38%\n",
            "\n",
            "Epoch 16/30\n",
            "Training average loss: 0.2880\n",
            "Validation Loss: 0.1996, Validation Accuracy: 93.40%\n",
            "\n",
            "Epoch 17/30\n",
            "Training average loss: 0.2646\n",
            "Validation Loss: 0.1845, Validation Accuracy: 93.68%\n",
            "\n",
            "Epoch 18/30\n",
            "Training average loss: 0.2506\n",
            "Validation Loss: 0.1758, Validation Accuracy: 94.42%\n",
            "\n",
            "Epoch 19/30\n",
            "Training average loss: 0.2396\n",
            "Validation Loss: 0.1888, Validation Accuracy: 94.42%\n",
            "\n",
            "Epoch 20/30\n",
            "Training average loss: 0.2280\n",
            "Validation Loss: 0.1707, Validation Accuracy: 94.62%\n",
            "\n",
            "Epoch 21/30\n",
            "Training average loss: 0.2199\n",
            "Validation Loss: 0.1578, Validation Accuracy: 95.32%\n",
            "\n",
            "Epoch 22/30\n",
            "Training average loss: 0.2050\n",
            "Validation Loss: 0.1903, Validation Accuracy: 94.02%\n",
            "\n",
            "Epoch 23/30\n",
            "Training average loss: 0.2005\n",
            "Validation Loss: 0.1722, Validation Accuracy: 94.54%\n",
            "\n",
            "Epoch 24/30\n",
            "Training average loss: 0.1835\n",
            "Validation Loss: 0.1494, Validation Accuracy: 95.38%\n",
            "\n",
            "Epoch 25/30\n",
            "Training average loss: 0.1807\n",
            "Validation Loss: 0.1157, Validation Accuracy: 96.26%\n",
            "\n",
            "Epoch 26/30\n",
            "Training average loss: 0.1689\n",
            "Validation Loss: 0.1392, Validation Accuracy: 95.54%\n",
            "\n",
            "Epoch 27/30\n",
            "Training average loss: 0.1629\n",
            "Validation Loss: 0.1310, Validation Accuracy: 95.44%\n",
            "\n",
            "Epoch 28/30\n",
            "Training average loss: 0.1609\n",
            "Validation Loss: 0.1184, Validation Accuracy: 96.42%\n",
            "\n",
            "Epoch 29/30\n",
            "Training average loss: 0.1495\n",
            "Validation Loss: 0.1014, Validation Accuracy: 96.72%\n",
            "\n",
            "Epoch 30/30\n",
            "Training average loss: 0.1429\n",
            "Validation Loss: 0.1092, Validation Accuracy: 96.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_loader, model, loss_function)"
      ],
      "metadata": {
        "id": "EPzlwmwhlWPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2030d651-f41e-4968-b80e-4bfc83aef70a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3364, Test Accuracy: 90.84%\n"
          ]
        }
      ]
    }
  ]
}